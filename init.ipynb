{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "196efe32",
   "metadata": {},
   "source": [
    "# Import libraries and set/get AWS properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b4c5dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request, argparse\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Define IAM role, session, region, and prefix for data directory\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "prefix = 'sagemaker'\n",
    "region = boto3.session.Session().region_name # set the region of the instance\n",
    "\n",
    "# Set S3 bucket name to store data, as well as boto3 client for S3 services\n",
    "bucketName = 'tdcx-test'\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780d1ee",
   "metadata": {},
   "source": [
    "# ETL on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f221feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KDD Cup dataset using scikit-learn's dataset loader\n",
    "# Set subset to SF to get logged_in is possitive data, allowing us to detect abnormal data as intrusion attacks\n",
    "# Should have a 0.3% of abnormal connections\n",
    "rawData = fetch_kddcup99(subset = 'SF')\n",
    "\n",
    "# Split raw data up into train test split with 70/30 split of train/test data\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(rawData.data, rawData.target, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Add label columns to dataset as we will be loading them into our S3 bucket to store said data\n",
    "trainX = pd.DataFrame(Xtrain, columns = rawData.feature_names)\n",
    "testX = pd.DataFrame(Xtest, columns = rawData.feature_names)\n",
    "\n",
    "# Isolation forest sets -1 as outliers and 1 as non-outliers, so we change all b'normal.' connections to 1 and every other\n",
    "# label as -1\n",
    "Ytrain = [1 if label == b'normal.' else -1 for label in Ytrain]\n",
    "Ytest = [1 if label == b'normal.' else -1 for label in Ytest]\n",
    "\n",
    "# Add labels to training/testing data\n",
    "trainX['labels'] = Ytrain\n",
    "testX['labels'] = Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b7fe1",
   "metadata": {},
   "source": [
    "# Setup training session on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1d6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training/testing data to csv files\n",
    "trainX.to_csv('train.csv', index = False, header = True)\n",
    "testX.to_csv('test.csv', index = False, header = True)\n",
    "\n",
    "# Create definition for input data used by Sagemaker training jobs\n",
    "# trainPath = sagemaker.inputs.TrainingInput(s3_data = 's3://{}/{}/train'.format(bucketName, prefix), content_type = 'csv')\n",
    "\n",
    "# Send data to S3 buckets\n",
    "trainPath = session.upload_data(path = 'train.csv', bucket = bucketName, key_prefix = prefix)\n",
    "testPath = session.upload_data(path = 'test.csv', bucket = bucketName, key_prefix = prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02eb1cb",
   "metadata": {},
   "source": [
    "# Create estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57aa51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scikit-learn estimator from Sagemaker's SDK\n",
    "estimator = SKLearn(\n",
    "    entry_point = 'trainingScript.py',\n",
    "    role = role,\n",
    "    train_instance_count = 1,\n",
    "    train_instance_type = 'ml.t2.medium',\n",
    "    framework_version = '0.20.0',\n",
    "    base_job_name = 'rf-scikit',\n",
    "    hyperparameters = {'n-estimators': 150}\n",
    ")\n",
    "\n",
    "# Fit estimator from S3 bucket data\n",
    "estimator.fit({'train': trainPath, 'test'L testPath}, wait = False)\n",
    "\n",
    "# Deploy estimator to endpoint and get predictor\n",
    "predictor = sklearn_estimator.deploy(instance_type = 'ml.t2.medium', initial_instance_count = 1)\n",
    "\n",
    "# Get response\n",
    "response = predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b59176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
